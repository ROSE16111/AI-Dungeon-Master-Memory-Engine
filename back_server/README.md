# Back Server - Real-time Speech Transcription
# Back æœåŠ¡ - å®æ—¶è¯­éŸ³è¯†åˆ«åç«¯

---

## ğŸ“– Introduction | ç®€ä»‹

**English**  
This project provides a WebSocket-based backend service for **real-time speech transcription**.  
It uses [FasterWhisper](https://github.com/SYSTRAN/faster-whisper) for transcription and [WebRTC VAD](https://github.com/wiseman/py-webrtcvad) for detecting voice activity.  
The server receives raw PCM16 audio data, transcribes speech, and sends **partial and final transcription results** back to the client in real time.

**ä¸­æ–‡**  
è¯¥é¡¹ç›®å®ç°äº†ä¸€ä¸ªåŸºäº **WebSocket çš„å®æ—¶è¯­éŸ³è¯†åˆ«åç«¯æœåŠ¡**ã€‚  
å®ƒä½¿ç”¨ [FasterWhisper](https://github.com/SYSTRAN/faster-whisper) è¿›è¡Œè½¬å½•ï¼Œå¹¶åˆ©ç”¨ [WebRTC VAD](https://github.com/wiseman/py-webrtcvad) æ¥æ£€æµ‹è¯­éŸ³æ´»åŠ¨ã€‚  
æœåŠ¡å™¨æ¥æ”¶ PCM16 åŸå§‹éŸ³é¢‘æ•°æ®ï¼Œå®æ—¶è¾“å‡º **éƒ¨åˆ†è½¬å½•ç»“æœ** å’Œ **æœ€ç»ˆè½¬å½•ç»“æœ** ç»™å®¢æˆ·ç«¯ã€‚

---

## ğŸ“‚ Project Structure | é¡¹ç›®ç»“æ„

```
back-server/
â”œâ”€â”€ src/back.py              # Main program / ä¸»ç¨‹åº
â”œâ”€â”€ requirements.txt         # Dependencies / ä¾èµ–æ–‡ä»¶
â”œâ”€â”€ README.md                # Documentation / æ–‡æ¡£
â”œâ”€â”€ .gitignore               # Ignore rules / Gitå¿½ç•¥è§„åˆ™
â””â”€â”€ LICENSE                  # License / è®¸å¯è¯
```

---

## ğŸ“¦ Installation | å®‰è£…

**English**
```bash
git clone https://github.com/YOUR_USERNAME/back-server.git
cd back-server
pip install -r requirements.txt
```

**ä¸­æ–‡**
```bash
å…‹éš†ä»“åº“å¹¶è¿›å…¥ç›®å½•ï¼š
git clone https://github.com/YOUR_USERNAME/back-server.git
cd back-server

å®‰è£…ä¾èµ–ï¼š
pip install -r requirements.txt
```

---

## ğŸš€ Run Server | è¿è¡ŒæœåŠ¡

**English**
```bash
python src/back.py
```

Server will run at:
```
ws://0.0.0.0:5000/audio
```

**ä¸­æ–‡**
```bash
python src/back.py
```

æœåŠ¡è¿è¡Œåœ¨ï¼š
```
ws://0.0.0.0:5000/audio
```

---

## ğŸ“¡ WebSocket Protocol | WebSocket åè®®

**English**
- Client sends: raw PCM16 audio chunks (mono, 16kHz).
- Server responds:
```json
{"partial": "hello wor"}
{"final": "hello world"}
```

**ä¸­æ–‡**
- å®¢æˆ·ç«¯å‘é€ï¼šPCM16 åŸå§‹éŸ³é¢‘ç‰‡æ®µï¼ˆå•å£°é“ï¼Œ16kHzï¼‰ã€‚
- æœåŠ¡å™¨è¿”å›ï¼š
```json
{"partial": "ä½ å¥½ï¼Œä¸–"}
{"final": "ä½ å¥½ï¼Œä¸–ç•Œ"}
```

---

## ğŸ§© Code Explanation | ä»£ç è§£é‡Š

### 1. Imports | å¯¼å…¥åº“
```python
import asyncio, websockets, json, queue, time
import numpy as np
import webrtcvad
from faster_whisper import WhisperModel
from datetime import datetime
```
**English**: Import libraries for async server, audio processing, and speech recognition.  
**ä¸­æ–‡**: å¯¼å…¥å¼‚æ­¥æœåŠ¡å™¨ã€éŸ³é¢‘å¤„ç†ã€è¯­éŸ³è¯†åˆ«æ‰€éœ€çš„åº“ã€‚

---

### 2. Configuration | é…ç½®å‚æ•°
```python
SAMPLE_RATE = 16000
CHANNELS = 1
FRAME_MS = 20
FRAME_SIZE = SAMPLE_RATE * FRAME_MS // 1000
SILENCE_END_MS = 600
PARTIAL_INTERVAL = 0.9
OVERLAP_SEC = 0.2
MODEL_NAME = "small"
LANG = "en"
BEAM = 1
TEMP = 0.0
```
**English**: Define constants for audio format, silence threshold, model type, and decoding settings.  
**ä¸­æ–‡**: å®šä¹‰éŸ³é¢‘æ ¼å¼ã€é™éŸ³é˜ˆå€¼ã€æ¨¡å‹å¤§å°ã€è§£ç å‚æ•°ç­‰ã€‚

---

### 3. Load Models | åŠ è½½æ¨¡å‹
```python
model = WhisperModel(MODEL_NAME, device="cpu", compute_type="int8")
vad = webrtcvad.Vad(2)
```
**English**: Load Whisper speech recognition model and WebRTC VAD for voice activity detection.  
**ä¸­æ–‡**: åŠ è½½ Whisper è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œä»¥åŠ WebRTC VAD ç”¨äºæ£€æµ‹æ˜¯å¦æœ‰è¯­éŸ³ã€‚

---

### 4. Voice Detection | è¯­éŸ³æ£€æµ‹
```python
def is_speech_int16(frames_int16: np.ndarray) -> bool:
    return vad.is_speech(frames_int16.tobytes(), SAMPLE_RATE)
```
**English**: Detect if audio contains speech.  
**ä¸­æ–‡**: åˆ¤æ–­å½“å‰éŸ³é¢‘ç‰‡æ®µæ˜¯å¦åŒ…å«è¯­éŸ³ã€‚

---

### 5. Transcription | è½¬å½•
```python
def transcribe_float32(wave_f32: np.ndarray) -> str:
    segments, _ = model.transcribe(
        wave_f32, language=LANG, beam_size=BEAM, temperature=TEMP
    )
    return "".join(seg.text for seg in segments).strip()
```
**English**: Convert audio waveform into text using FasterWhisper.  
**ä¸­æ–‡**: ä½¿ç”¨ FasterWhisper å°†éŸ³é¢‘æ³¢å½¢è½¬å½•ä¸ºæ–‡å­—ã€‚

---

### 6. WebSocket Handler | WebSocket å¤„ç†é€»è¾‘
```python
async def audio_handler(websocket):
    async for message in websocket:
        pcm16 = np.frombuffer(message, dtype=np.int16)
        voiced = is_speech_int16(pcm16)
        # handle speech and silence...
```
**English**: Handle audio chunks from client, detect speech, perform transcription, and send results back.  
**ä¸­æ–‡**: å¤„ç†å®¢æˆ·ç«¯ä¼ æ¥çš„éŸ³é¢‘ç‰‡æ®µï¼Œæ£€æµ‹è¯­éŸ³ï¼Œè½¬å½•å¹¶è¿”å›ç»“æœã€‚

---

### 7. Start Server | å¯åŠ¨æœåŠ¡
```python
async def main():
    async with websockets.serve(audio_handler, "0.0.0.0", 5000):
        await asyncio.Future()
```
**English**: Start WebSocket server at port 5000.  
**ä¸­æ–‡**: å¯åŠ¨ WebSocket æœåŠ¡ï¼Œç›‘å¬ 5000 ç«¯å£ã€‚

---

## ğŸ› ï¸ How to Modify & Debug | å¦‚ä½•ä¿®æ”¹ä¸è°ƒè¯•

**English**
- Change `MODEL_NAME` (`tiny`, `base`, `small`, `medium`, `large`) to adjust recognition speed/accuracy.  
- Adjust `SILENCE_END_MS` to detect end of speech faster/slower.  
- Modify `LANG` to force recognition language.  
- Debug by printing intermediate values (e.g., `print(pcm16.shape)`).  
- Use `device="cuda"` if you have a GPU to speed up transcription.

**ä¸­æ–‡**
- ä¿®æ”¹ `MODEL_NAME` (`tiny`, `base`, `small`, `medium`, `large`) æ¥å¹³è¡¡è¯†åˆ«é€Ÿåº¦å’Œå‡†ç¡®ç‡ã€‚  
- è°ƒæ•´ `SILENCE_END_MS` ä»¥æ”¹å˜è¯­éŸ³ç»“æŸæ£€æµ‹çš„çµæ•åº¦ã€‚  
- ä¿®æ”¹ `LANG` æ¥å¼ºåˆ¶è¯†åˆ«æŒ‡å®šè¯­è¨€ã€‚  
- è°ƒè¯•æ—¶å¯ä»¥æ‰“å°ä¸­é—´å˜é‡ï¼ˆä¾‹å¦‚ `print(pcm16.shape)`ï¼‰ã€‚  
- å¦‚æœæœ‰ GPUï¼Œå¯å°† `device="cuda"` æé«˜è¯†åˆ«é€Ÿåº¦ã€‚  

---
